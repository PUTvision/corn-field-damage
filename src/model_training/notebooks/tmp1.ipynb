{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797080ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc795b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_DEVICE = 'cpu'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# DEVICE = 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d30941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01091ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cf48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f70ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_BIG_STRIDE = True  # if using images with big stride (768 instead of 256 pixels). Faster training and validation/test on random samples from all files\n",
    "\n",
    "\n",
    "if IS_BIG_STRIDE:\n",
    "    TILES_BASE_DIR = \"/media/data/local/corn/processed_stride768\"\n",
    "    SUBDIRECTORIES_TO_PROCESS_TRAIN = [\n",
    "        \"kukurydza_5_ha\",\n",
    "#         \"kukurydza_10_ha\",\n",
    "        \"kukurydza_11_ha\",\n",
    "        \"kukurydza_13_ha\",\n",
    "        \"kukurydza_15_ha\",\n",
    "        \"kukurydza_18_ha\",\n",
    "        \"kukurydza_25_ha\",\n",
    "        \"kukurydza_38_ha\",\n",
    "        \"kukurydza_60_ha\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    # if TEST or VALIDATIONS are empty, random part of training set will be used\n",
    "    SUBDIRECTORIES_TO_PROCESS_VALID = [\n",
    "    ]\n",
    "\n",
    "    SUBDIRECTORIES_TO_PROCESS_TEST = [\n",
    "    ]\n",
    "#     SUBDIRECTORIES_TO_PROCESS_TRAIN = [\n",
    "#         \"kukurydza_5_ha\",\n",
    "# #         \"kukurydza_10_ha\",\n",
    "# #         \"kukurydza_11_ha\",\n",
    "#         \"kukurydza_13_ha\",\n",
    "#         \"kukurydza_15_ha\",\n",
    "#         \"kukurydza_18_ha\",\n",
    "#         \"kukurydza_25_ha\",\n",
    "#         \"kukurydza_38_ha\",\n",
    "#         \"kukurydza_60_ha\",\n",
    "#     ]\n",
    "\n",
    "\n",
    "#     # if TEST or VALIDATIONS are empty, random part of training set will be used\n",
    "#     SUBDIRECTORIES_TO_PROCESS_VALID = [\n",
    "#         \"kukurydza_11_ha\",\n",
    "#     ]\n",
    "\n",
    "#     SUBDIRECTORIES_TO_PROCESS_TEST = [\n",
    "#         \"kukurydza_10_ha\",    \n",
    "#     ]\n",
    "\n",
    "\n",
    "UNCROPPED_TILE_SIZE = (512 + 256)  # in pixels\n",
    "CROPPED_TILE_SIZE = 512\n",
    "CROP_TILE_MARGIN = (UNCROPPED_TILE_SIZE - CROPPED_TILE_SIZE) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TilesPaths:\n",
    "    img_paths: List = field(default_factory=lambda:[])\n",
    "    mask_paths: List = field(default_factory=lambda:[])\n",
    "\n",
    "    def split_into_train_valid_test(self, percentage_for_train=0.8):\n",
    "        N = len(self.img_paths)\n",
    "        test_percentage = (1 - percentage_for_train) / 2\n",
    "        sp = [int(N*percentage_for_train), int(N*(1-test_percentage))]  # dataset split points\n",
    "        tile_paths_train = TilesPaths(img_paths=self.img_paths[:sp[0]], mask_paths=self.mask_paths[:sp[0]])\n",
    "        tile_paths_valid = TilesPaths(img_paths=self.img_paths[sp[0]:sp[1]], mask_paths=self.mask_paths[sp[0]:sp[1]])\n",
    "        tile_paths_test = TilesPaths(img_paths=self.img_paths[sp[1]:], mask_paths=self.mask_paths[sp[1]:])\n",
    "        return tile_paths_train, tile_paths_valid, tile_paths_test\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        new = copy.deepcopy(self)\n",
    "        new.img_paths += other.img_paths\n",
    "        new.mask_paths += other.mask_paths\n",
    "        return new\n",
    "    \n",
    "    def shuffle(self):\n",
    "        c = list(zip(self.img_paths, self.mask_paths))\n",
    "        random.shuffle(c)\n",
    "        self.img_paths, self.mask_paths = zip(*c)\n",
    "\n",
    "    \n",
    "def get_tile_paths_for_directories_with_split(dir_names):\n",
    "    tile_paths_train = TilesPaths()\n",
    "    tile_paths_valid = TilesPaths()\n",
    "    tile_paths_test = TilesPaths()\n",
    "    for dir_name in dir_names:\n",
    "        all_tile_paths = get_tile_paths_for_directories(dir_names=[dir_name])\n",
    "        new_train, new_valid, new_test = all_tile_paths.split_into_train_valid_test()\n",
    "        tile_paths_train += new_train\n",
    "        tile_paths_valid += new_valid\n",
    "        tile_paths_test += new_test\n",
    "    \n",
    "    tile_paths_train.shuffle()\n",
    "    tile_paths_valid.shuffle()\n",
    "    tile_paths_test.shuffle()\n",
    "    \n",
    "    return tile_paths_train, tile_paths_valid, tile_paths_test\n",
    "    \n",
    "        \n",
    "def get_tile_paths_for_directories(dir_names, shuffle=True) -> TilesPaths:\n",
    "    tile_paths = TilesPaths()\n",
    "    for dir_name in dir_names:\n",
    "        dir_path = os.path.join(TILES_BASE_DIR, dir_name)\n",
    "        file_names = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "        mask_files_prefixes = set([f[:f.rfind('_')] for f in file_names if 'mask' in f])\n",
    "        img_files_prefixes = set([f[:f.rfind('_')] for f in file_names if 'img' in f])\n",
    "        common_files_prefixes = mask_files_prefixes.intersection(img_files_prefixes)\n",
    "        all_files_prefixes = mask_files_prefixes.union(img_files_prefixes)\n",
    "        missing_files_prefixes = all_files_prefixes - common_files_prefixes\n",
    "\n",
    "        if missing_files_prefixes:\n",
    "            raise Exception(f\"Some files don't have correponding pair in mask/image: {missing_files_prefixes} in {dir_path}\")\n",
    "\n",
    "        common_files_prefixes = list(common_files_prefixes)\n",
    "        if shuffle:\n",
    "            random.shuffle(common_files_prefixes)\n",
    "        for file_prefix in common_files_prefixes:\n",
    "            img_file_name = file_prefix + '_img.png'\n",
    "            mask_file_name = file_prefix + '_mask.png'\n",
    "            tile_paths.img_paths.append(os.path.join(dir_path, img_file_name))\n",
    "            tile_paths.mask_paths.append(os.path.join(dir_path, mask_file_name))\n",
    "    return tile_paths\n",
    "\n",
    "\n",
    "if SUBDIRECTORIES_TO_PROCESS_VALID and SUBDIRECTORIES_TO_PROCESS_TEST:\n",
    "    # we have valid tiles for test/valid\n",
    "    tile_paths_train = get_tile_paths_for_directories(SUBDIRECTORIES_TO_PROCESS_TRAIN)\n",
    "    tile_paths_valid = get_tile_paths_for_directories(SUBDIRECTORIES_TO_PROCESS_VALID)\n",
    "    tile_paths_test = get_tile_paths_for_directories(SUBDIRECTORIES_TO_PROCESS_TEST)\n",
    "else:\n",
    "    tile_paths_train, tile_paths_valid, tile_paths_test = get_tile_paths_for_directories_with_split(SUBDIRECTORIES_TO_PROCESS_TRAIN)\n",
    "\n",
    "\n",
    "print(f'Number of tiles train = {len(tile_paths_train.img_paths)}')\n",
    "print(f'Number of tiles validation = {len(tile_paths_valid.img_paths)}')\n",
    "print(f'Number of tiles test = {len(tile_paths_test.img_paths)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c840393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7464b6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690666cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462872e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a09eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43682015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbd19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTATION_CLASS_VALUES = [0, 255, 127]\n",
    "NUMBER_OF_SEGMENTATION_CLASSES = len(SEGMENTATION_CLASS_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a38360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CornFieldDamageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_file_paths, mask_file_paths, augment=True):\n",
    "        self.img_file_paths = img_file_paths\n",
    "        self.mask_file_paths = mask_file_paths\n",
    "        assert(len(self.img_file_paths) == len(mask_file_paths))\n",
    "        if augment:\n",
    "            self._img_and_mask_transform = self._get_img_and_mask_augmentation_tranform()  # augmentation transform\n",
    "        else:\n",
    "            self._img_and_mask_transform = self._get_img_and_mask_crop_tranform()  # crop only transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image = cv2.imread(self.img_file_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # not really needed I guess\n",
    "        mask = cv2.imread(self.mask_file_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        transformed = self._img_and_mask_transform(image=image, mask=mask)\n",
    "        image, mask = transformed['image'], transformed['mask']\n",
    "        \n",
    "        masks = [(mask == v) for v in SEGMENTATION_CLASS_VALUES]\n",
    "        mask_stacked = np.stack(masks, axis=0).astype('float')\n",
    "        \n",
    "        image = image.astype('float')\n",
    "        image /= 255\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        return image.astype('float32'), mask_stacked.astype('float32')\n",
    "        \n",
    "    def _get_img_and_mask_augmentation_tranform(self):\n",
    "        # Declare an augmentation pipeline\n",
    "        transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomScale(scale_limit=0.15),  # above scale 0.16 images are too small\n",
    "            A.Rotate(limit=90),  # degrees\n",
    "            # TODO normalize instead divide by 255?\n",
    "            A.Crop(x_min=CROP_TILE_MARGIN, y_min=CROP_TILE_MARGIN, x_max=UNCROPPED_TILE_SIZE-CROP_TILE_MARGIN, y_max=UNCROPPED_TILE_SIZE-CROP_TILE_MARGIN),\n",
    "            # TODO ToTensorV2 instead of manual stacking and transpoition?\n",
    "        ])\n",
    "        # TODO - color, contrast, gamma, randomShadow, rain\n",
    "        return transform\n",
    "\n",
    "    def _get_img_and_mask_crop_tranform(self):\n",
    "        transform = A.Compose([\n",
    "            A.Crop(x_min=CROP_TILE_MARGIN, y_min=CROP_TILE_MARGIN, x_max=UNCROPPED_TILE_SIZE-CROP_TILE_MARGIN, y_max=UNCROPPED_TILE_SIZE-CROP_TILE_MARGIN),\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "train_dataset = CornFieldDamageDataset(img_file_paths=tile_paths_train.img_paths, mask_file_paths=tile_paths_train.mask_paths)\n",
    "valid_dataset = CornFieldDamageDataset(img_file_paths=tile_paths_valid.img_paths, mask_file_paths=tile_paths_valid.mask_paths, augment=False)\n",
    "test_dataset = CornFieldDamageDataset(img_file_paths=tile_paths_test.img_paths, mask_file_paths=tile_paths_test.mask_paths, augment=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=6, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c0d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b728f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, mask = train_dataset[222] # get some sample\n",
    "# plt.imshow(mask[0, :, :])\n",
    "# plt.show()\n",
    "# plt.imshow(mask[1, :, :])\n",
    "# plt.show()\n",
    "# plt.imshow(mask[2, :, :])\n",
    "# plt.show()\n",
    "# plt.imshow(image.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e00230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=NUMBER_OF_SEGMENTATION_CLASSES,  # model output channels (number of classes in your dataset)\n",
    "    activation='softmax2d',  # ?\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb63f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()  # class imbalance is typically taken care of simply by assigning loss multipliers to each class,\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5, name='IoU'),\n",
    "    smp.utils.metrics.IoU(threshold=0.5, ignore_channels=[1, 2], name='IoU-0'),\n",
    "    smp.utils.metrics.IoU(threshold=0.5, ignore_channels=[0, 2], name='IoU-1'),\n",
    "    smp.utils.metrics.IoU(threshold=0.5, ignore_channels=[0, 1], name='IoU-2'),\n",
    "    smp.utils.metrics.Fscore(threshold=0.5, ignore_channels=[2]),\n",
    "    smp.utils.metrics.Accuracy(threshold=0.5, ignore_channels=[2]),\n",
    "    smp.utils.metrics.Recall(threshold=0.5, ignore_channels=[2]),\n",
    "    smp.utils.metrics.Precision(threshold=0.5, ignore_channels=[2]),\n",
    "]\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model_fnn.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam([ \n",
    "#     dict(params=model.parameters(), lr=(0.0001)),  # 0.0001  #   0.000003 gives 80 epoch for 768 stride\n",
    "    dict(params=model.parameters(), lr=(0.000005 if IS_BIG_STRIDE else 0.00001)),  # 0.0001  #   0.000003 gives 80 epoch for 768 stride\n",
    "])\n",
    "\n",
    "\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "for e in [valid_epoch, train_epoch]:\n",
    "    e.metrics[1].__name__=\"IoU_Class0\"\n",
    "    e.metrics[2].__name__=\"IoU_Class1\"\n",
    "    e.metrics[3].__name__=\"IoU_Class2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd386a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5932ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dca010",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "train_logs_vec = []\n",
    "valid_logs_vec = []\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315102df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_epochs = 55 if IS_BIG_STRIDE else 4\n",
    "epoch_to_decrease_learning_rate = 40 if IS_BIG_STRIDE else 2\n",
    "\n",
    "for i in range(0, number_of_epochs):\n",
    "    print(f'\\nEpoch: {i}')\n",
    "    train_logs_vec.append(train_epoch.run(train_loader))\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    valid_logs_vec.append(valid_logs)\n",
    "\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        model.to(CPU_DEVICE)\n",
    "        best_model = copy.deepcopy(model)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "    if i == epoch_to_decrease_learning_rate:\n",
    "        optimizer.param_groups[0]['lr'] /= 2\n",
    "        print(f\"Decrease decoder learning rate to {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c7625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbba8f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for metric in valid_logs_vec[0].keys():\n",
    "    train_metric_vec = [m[metric] for m in train_logs_vec]\n",
    "    valid_metric_vec = [m[metric] for m in valid_logs_vec]\n",
    "    \n",
    "    plt.plot(train_metric_vec)\n",
    "    plt.plot(valid_metric_vec)\n",
    "\n",
    "    plt.legend(['train', 'valid'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logs_vec[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e199f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model\n",
    "model.to(DEVICE)\n",
    "\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "test_epoch.run(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')                 \n",
    "torch.save(model.state_dict(), '/media/data/local/corn/processed_stride768//model_cpu__trained_without_10ha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b372e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c1b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad7193",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vi = iter(test_loader)\n",
    "# vi = iter(train_loader)\n",
    "\n",
    "for i in range(8):  # increase to get more images\n",
    "    img_batch, mask_batch = next(vi)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    #     model_output = model(img_batch.to(DEVICE))\n",
    "        model_output = model(img_batch)\n",
    "\n",
    "\n",
    "    columns = 5\n",
    "    rows = len(img_batch)\n",
    "    fig = plt.figure(figsize=(columns * 4, rows * 4))\n",
    "\n",
    "\n",
    "    for i in range(len(img_batch)):\n",
    "        fig.add_subplot(rows, columns, 1 + i*columns + 0)   \n",
    "        plt.imshow(img_batch[i].numpy().transpose([1, 2, 0]))\n",
    "        plt.axis('off')\n",
    "        plt.title('img')\n",
    "\n",
    "        fig.add_subplot(rows, columns, 1 + i*columns + 1)   \n",
    "        plt.imshow(mask_batch[i][1].numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title('original damage mask')\n",
    "\n",
    "        fig.add_subplot(rows, columns, 1 + i*columns + 2)   \n",
    "        plt.imshow(model_output[i][1])\n",
    "        plt.axis('off')\n",
    "        plt.title('prediction damage')\n",
    "\n",
    "        fig.add_subplot(rows, columns, 1 + i*columns + 3)   \n",
    "        cax = plt.imshow(model_output[i][1] - mask_batch[i][1], vmin=-1.1, vmax=1.1)\n",
    "        plt.title('damage diff (predict-gt)')\n",
    "        plt.axis('off')\n",
    "        cbar = fig.colorbar(cax, ticks=[-1, 0, 1])\n",
    "        cbar.ax.set_yticklabels(['false negative', 'true', 'false positive'])\n",
    "\n",
    "        fig.add_subplot(rows, columns, 1 + i*columns + 4)   \n",
    "        plt.imshow(model_output[i][0])\n",
    "        plt.title('prediction healty field')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e55787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee2135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c027d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'  \n",
    "# device = DEVICE\n",
    "\n",
    "model = model.to(device)\n",
    "number_of_batches = len(test_loader)\n",
    "\n",
    "\n",
    "healthy_field_ground_truth_pix = 0\n",
    "damage_ground_truth_pix = 0\n",
    "\n",
    "healthy_field_predicted_pix = 0\n",
    "damage_field_predicted_pix = 0\n",
    "\n",
    "damage_prediction_true_positives_pix = 0\n",
    "\n",
    "\n",
    "healthy_intersection_pix = 0\n",
    "healthy_union_pix = 0\n",
    "\n",
    "damage_intersection_pix = 0\n",
    "damage_union_pix = 0\n",
    "\n",
    "\n",
    "\n",
    "for i, (img_batch, mask_batch) in enumerate(test_loader):\n",
    "    print(f'Batch {i} / {number_of_batches}')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(img_batch.to(device)).to(CPU_DEVICE)\n",
    "    \n",
    "    for i in range(model_output.shape[0]):\n",
    "        ground_truth_healthy_field = mask_batch[i, 0, :, :].numpy().astype(int)\n",
    "        ground_truth_damage = mask_batch[i, 1, :, :].numpy().astype(int)\n",
    "\n",
    "        predicted_healty_field = model_output[i, 0, :, :].numpy()\n",
    "        predicted_damage = model_output[i, 1, :, :].numpy()\n",
    "        predicted_healty_field = np.where(predicted_healty_field > 0.5, 1, 0)\n",
    "        predicted_damage = np.where(predicted_damage > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "        healthy_field_ground_truth_pix += np.count_nonzero(ground_truth_healthy_field)\n",
    "        damage_ground_truth_pix += np.count_nonzero(ground_truth_damage)\n",
    "\n",
    "        healthy_field_predicted_pix += np.count_nonzero(predicted_healty_field)\n",
    "        damage_field_predicted_pix += np.count_nonzero(predicted_damage)\n",
    "\n",
    "        common_damage = np.logical_and(ground_truth_damage, predicted_damage)\n",
    "        damage_prediction_true_positives_pix += np.count_nonzero(common_damage)\n",
    "        \n",
    "        common_healthy = np.logical_and(ground_truth_healthy_field, predicted_healty_field)\n",
    "        damage_intersection_pix += np.count_nonzero(common_damage)\n",
    "        healthy_intersection_pix += np.count_nonzero(common_healthy)\n",
    "        damage_union_pix += np.count_nonzero(np.logical_or(ground_truth_damage, predicted_damage))\n",
    "        healthy_union_pix += np.count_nonzero(np.logical_or(ground_truth_healthy_field, predicted_healty_field))\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "total_ground_truth_pix = healthy_field_ground_truth_pix + damage_ground_truth_pix\n",
    "total_predicted_pix = healthy_field_predicted_pix + damage_field_predicted_pix\n",
    "\n",
    "iou_damage = damage_intersection_pix / damage_union_pix\n",
    "iou_healthy = healthy_intersection_pix / healthy_union_pix\n",
    "\n",
    "print(f'healthy_field_ground_truth = {healthy_field_ground_truth_pix / total_ground_truth_pix * 100:.2f} %')\n",
    "print(f'damage_ground_truth = {damage_ground_truth_pix / total_ground_truth_pix * 100:.2f} %')\n",
    "\n",
    "print(f'healthy_field_predicted = {healthy_field_predicted_pix / total_predicted_pix * 100:.2f} %')\n",
    "print(f'damage_field_predicted = {damage_field_predicted_pix / total_predicted_pix * 100:.2f} %')\n",
    "\n",
    "print(f'damage_prediction_true_positives/damage_field_predicted = {damage_prediction_true_positives_pix / damage_field_predicted_pix * 100:.2f} %')\n",
    "\n",
    "print(f'iou_damage = {iou_damage:.3f}')\n",
    "print(f'iou_healthy = {iou_healthy:.3f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15f9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e79ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542a5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7d9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e205856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aedcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
